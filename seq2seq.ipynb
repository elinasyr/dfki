{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def plan_from_trajectories(t, keep_only_order=False):\n",
    "  plan = []\n",
    "  prev_goal = None\n",
    "\n",
    "  for goal in t['goal_status']:\n",
    "    if goal != prev_goal:\n",
    "      plan.append(goal)\n",
    "      prev_goal = goal\n",
    "  # Remove duplicates while preserving order\n",
    "  final_plan = list(dict.fromkeys(plan))\n",
    "  if keep_only_order: \n",
    "    plan_by_numbers = []\n",
    "    for stop in final_plan: \n",
    "      if stop[:6] == 'moving': \n",
    "          plan_by_numbers.append(int(stop[-1]))\n",
    "    return plan_by_numbers\n",
    "  return final_plan\n",
    "\n",
    "plan_mapping = {'[1, 6, 4, 2, 3, 5]': 1, '[3, 5, 4, 1, 6, 2]': 2, '[5, 4, 6, 3, 1, 2]': 3, '[2, 6, 1, 5, 4, 3]': 4, '[6, 5, 2, 4, 1, 3]': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter  \n",
    "tdata = pd.DataFrame()\n",
    "id = 0\n",
    "for i in range(10):\n",
    "  temp = pd.read_csv(f'./output_robot3data/out{i}.csv')\n",
    "  temp = temp[['goal_status']] # 'idle'\n",
    "  # temp = temp[::8].reset_index(drop=True)\n",
    "  # if len(Counter(temp['goal_status']).keys()) < 12:\n",
    "  #   continue\n",
    "  temp.drop_duplicates(subset=['goal_status'], keep='first', inplace=True)\n",
    "  if temp.shape[0]<10: \n",
    "    continue\n",
    "  temp['id'] = i\n",
    "  if temp.iloc[-1].goal_status == '(unknown)':\n",
    "    temp = temp[:-1]\n",
    "  temp['plan'] = plan_mapping[str(plan_from_trajectories(temp, keep_only_order=True))]\n",
    "  tdata = pd.concat([tdata, temp])\n",
    "  \n",
    "tdata.reset_index(drop=True, inplace=True)\n",
    "num_predictions=tdata['id'].unique().__len__()\n",
    "enc = LabelEncoder()\n",
    "\n",
    "tdata['goal_status'] = enc.fit_transform(tdata[['goal_status']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_seq_length(data):\n",
    "#   for id in data['id'].unique():\n",
    "#     print(f'{id}: {len(data[data[\"id\"] == id])}')\n",
    "#     print(Counter(data[data[\"id\"] == id]['goal_status']))\n",
    "# check_seq_length(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elinasyrr/.conda/envs/pytorch_forecasting/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:110: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter  \n",
    "data = pd.DataFrame()\n",
    "id = 0\n",
    "valid_plans = [1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 25, 32, 35, 40, 55]\n",
    "for i in valid_plans:\n",
    "  temp = pd.read_csv(f'./output_robot5data/out{i}.csv')\n",
    "  temp = temp[['goal_status']] # 'idle'\n",
    "  temp.drop_duplicates(subset=['goal_status'], keep='first', inplace=True)\n",
    "  if temp.shape[0]<10: \n",
    "    continue\n",
    "  plan = plan_from_trajectories(temp, keep_only_order=True)\n",
    "  # # print(plan)\n",
    "  if str(plan) not in list(plan_mapping.keys()):\n",
    "    continue\n",
    "  # temp = temp[::8].reset_index(drop=True)\n",
    "  # if len(Counter(temp['goal_status']).keys()) < 12:\n",
    "  #   continue\n",
    "  # if temp.shape[0] < 200: \n",
    "  #   continue\n",
    "  # print(Counter(temp['goal_status']).keys())\n",
    "  temp['id'] = i\n",
    "  temp['plan'] = plan_mapping[str(plan)]\n",
    "  if temp.iloc[-1].goal_status == '(unknown)':\n",
    "    temp = temp[:-1]\n",
    "\n",
    "  data = pd.concat([data, temp])\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "num_predictions=data['id'].unique().__len__()\n",
    "enc = LabelEncoder()\n",
    "\n",
    "data['goal_status'] = enc.fit_transform(data[['goal_status']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for id in data[data['plan'] == 3]['id'].unique():\n",
    "#   # plot the data in a scatter plot \n",
    "#   temp = data[data['id'] == id].goal_status.values\n",
    "#   print(temp)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Assuming your data is in a pandas DataFrame called 'data'\n",
    "# # and has columns 'id', 'plan', and 'goal_status'\n",
    "# plt.figure(figsize=(8, 6))  # Adjust figure size as needed\n",
    "\n",
    "# plan_id = 1\n",
    "# all_ids = data[data['plan'] == plan_id]['id'].unique()\n",
    "# num_ids = len(all_ids)\n",
    "# offset_scale = 0.1  # Adjust this value to control the offset\n",
    "\n",
    "# for i, id_val in enumerate(all_ids):\n",
    "#     goal_statuses = data[data['id'] == id_val]['goal_status']\n",
    "#     x_values = range(len(goal_statuses))\n",
    "#     offset = (i - num_ids / 2) * offset_scale  # Create a unique offset for each ID\n",
    "#     plt.scatter(x_values, goal_statuses + offset, label=f'ID: {id_val}', marker='o', linestyle='-')\n",
    "\n",
    "# plt.xlabel(\"Order of Goal Status\")\n",
    "# plt.ylabel(\"Goal Status Value (with Offset)\")\n",
    "# plt.title(f\"Goal Status Progression for Plan: {plan_id}\")\n",
    "# plt.xticks(x_values)\n",
    "# plt.yticks(sorted(data['goal_status'].unique())) # Show unique goal status values\n",
    "# plt.legend(title='ID', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "horizon = 3\n",
    "def prepare_data(data):\n",
    "  col = 'goal_status' #list(data.columns[1:])\n",
    "  X, y = [], []\n",
    "  for id in data['id'].unique():\n",
    "    t = data[data['id'] == id].drop(['id'], axis=1)\n",
    "    X.append(np.array(t[:-horizon][col]))\n",
    "    y.append(np.array(t[-horizon:][col]))\n",
    "  return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming data_encoded is your DataFrame\n",
    "X_train_raw, y_train_raw = prepare_data(data)\n",
    "X_test_raw, y_test_raw = prepare_data(tdata)\n",
    "# Find the maximum length in X_train_raw\n",
    "max_length = max(len(x) for x in X_train_raw)\n",
    "tmax_length = max(len(x) for x in X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sequences in X_train_raw\n",
    "import torch \n",
    "\n",
    "input = torch.tensor(y_train_raw, dtype=torch.long)\n",
    "X_train_tensor = [torch.tensor(arr, dtype=torch.long) for arr in X_train_raw]\n",
    "X_train_tensor = torch.nn.utils.rnn.pad_sequence(X_train_tensor, padding_side='left', batch_first=True).reshape(len(data['id'].unique()), max_length)\n",
    "\n",
    "tinput = torch.tensor(y_test_raw, dtype=torch.long)\n",
    "X_test_tensor = [torch.tensor(arr, dtype=torch.long) for arr in X_test_raw]\n",
    "X_test_tensor = torch.nn.utils.rnn.pad_sequence(X_test_tensor, padding_side='left',batch_first=True).reshape(len(tdata['id'].unique()), tmax_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EncoderRNN(nn.Module): \n",
    "  def __init__(self, input_size, hidden_size):\n",
    "    super(EncoderRNN, self).__init__()\n",
    "    self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "    \n",
    "  def forward(self, input_x):\n",
    "    o, h = self.gru(input_x)\n",
    "    return o,h\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "  def __init__(self, hidden_size, output_size):\n",
    "    super(DecoderRNN, self).__init__()\n",
    "    self.gru = nn.GRU(output_size, hidden_size, batch_first=True)\n",
    "    self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "  def forward(self, encoder_hidden, target_tensor): \n",
    "    output, hidden = self.gru(target_tensor, encoder_hidden)\n",
    "    output_logits = self.out(output)\n",
    "    return output_logits, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "def evaluate_model(encoder, decoder, test_loader, device, target_length=400):\n",
    "  encoder.eval()\n",
    "  decoder.eval()\n",
    "  all_predictions = []\n",
    "  all_labels = []  \n",
    "  with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "      batch_size_current = batch_x.size(0)\n",
    "      encoded_input = torch.nn.functional.one_hot(batch_x, num_classes=16).float().to(device)\n",
    "      batch_predictions = torch.zeros(batch_size_current, target_length, dtype=torch.long).to(device)\n",
    "      encoder_outputs, encoder_hidden = encoder(encoded_input)\n",
    "      decoder_hidden = encoder_hidden\n",
    "      decoder_input = torch.zeros(batch_size_current, 1, 16).to(device)\n",
    "      for t in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_hidden, decoder_input)\n",
    "        predicted_token = torch.argmax(decoder_output[:, -1, :], dim=1)\n",
    "        batch_predictions[:, t] = predicted_token\n",
    "        next_input_token = torch.nn.functional.one_hot(predicted_token, num_classes=16).unsqueeze(1).float()\n",
    "        decoder_input = torch.cat((decoder_input, next_input_token), dim=1)\n",
    "        decoder_hidden = decoder_hidden\n",
    "      for m in range(batch_size_current):\n",
    "        pred_li = list(int(num) for num in batch_predictions[m].cpu().numpy())\n",
    "        print(f'Prediction: {enc.inverse_transform(pred_li)}')\n",
    "        true_li = batch_y[m].cpu().numpy()\n",
    "        print(f'Actual: {enc.inverse_transform(true_li)}\\n')\n",
    "      all_predictions.extend(batch_predictions.cpu().flatten().numpy())\n",
    "      all_labels.extend(batch_y.flatten().numpy())\n",
    "    all_predictions = [int(x) for x in all_predictions]\n",
    "    all_labels = [int(x) for x in all_labels]\n",
    "    print(all_predictions[:100])\n",
    "    print(all_labels[:100])\n",
    "  f1_macro = f1_score(all_labels, all_predictions, average='macro')\n",
    "  cm = confusion_matrix(all_labels, all_predictions)    \n",
    "  cr = classification_report(all_labels, all_predictions)\n",
    "  print(f'Classification Report:\\n{cr}')\n",
    "  print(f'Confusion Matrix:\\n{cm}')\n",
    "  print(f'F1 Macro: {f1_macro}')\n",
    "  return np.array(all_labels), np.array(all_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_baseline(test_loader, target_length=100):\n",
    "    \"\"\"\n",
    "    Evaluates a baseline model that extrapolates the last value of the input.\n",
    "    \"\"\"\n",
    "    all_baseline_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_size_current = batch_x.size(0)\n",
    "            baseline_predictions = torch.zeros(batch_size_current, target_length, dtype=torch.long)\n",
    "            print(batch_x.shape)\n",
    "            # Get the last value of the input sequence for each batch\n",
    "            last_input_values = batch_x[:, -1]  # Assuming batch_x is one-hot encoded\n",
    "            print(last_input_values)\n",
    "            # Extrapolate the last value for the target length\n",
    "            for t in range(target_length):\n",
    "                baseline_predictions[:, t] = last_input_values\n",
    "            # for m in range(batch_size_current):\n",
    "            #   print(f'Prediction: {baseline_predictions[m].cpu().numpy()}')\n",
    "            #   print(f'Actual: {batch_y[m].cpu().numpy()}')\n",
    "\n",
    "            all_baseline_predictions.extend(baseline_predictions.cpu().flatten().numpy())\n",
    "            all_labels.extend(batch_y.flatten().numpy())\n",
    "\n",
    "    all_baseline_predictions = [int(x) for x in all_baseline_predictions]\n",
    "    all_labels = [int(x) for x in all_labels]\n",
    "\n",
    "    f1_macro_baseline = f1_score(all_labels, all_baseline_predictions, average='macro', zero_division=0)\n",
    "    cm_baseline = confusion_matrix(all_labels, all_baseline_predictions)\n",
    "    cr_baseline = classification_report(all_labels, all_baseline_predictions, zero_division=0)\n",
    "\n",
    "    print(\"\\n--- Baseline Evaluation (Last Value Extrapolation) ---\")\n",
    "    print(f'Classification Report:\\n{cr_baseline}')\n",
    "    print(f'Confusion Matrix:\\n{cm_baseline}')\n",
    "    print(f'F1 Macro: {f1_macro_baseline}')\n",
    "\n",
    "    return np.array(all_labels), np.array(all_baseline_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "torch.manual_seed(42)\n",
    "hidden_size = 64\n",
    "batch_size = 10\n",
    "classes = 16\n",
    "n_epochs = 500\n",
    "learning_rate = 0.001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = EncoderRNN(input_size=classes, hidden_size=hidden_size).to(device)\n",
    "decoder = DecoderRNN(hidden_size=hidden_size, output_size=classes).to(device)\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "train_dataset = TensorDataset(X_train_tensor, input)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "loss_per_epoch=[]\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  total_loss = 0 \n",
    "  for batch_x, batch_y in train_loader:\n",
    "    padded = torch.cat(\n",
    "    (\n",
    "        torch.zeros(batch_size, 1, 16),\n",
    "        torch.nn.functional.one_hot(batch_y, num_classes=16),\n",
    "    ),\n",
    "    dim=1,\n",
    "    )[:, :-1, :]\n",
    "    encoded_input=torch.nn.functional.one_hot(batch_x, num_classes=16).float()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    encoder_outputs, encoder_hidden = encoder(encoded_input.to(device))\n",
    "    decoder_outputs, _ = decoder(encoder_hidden, padded.to(device))\n",
    "    loss = torch.nn.functional.cross_entropy(decoder_outputs.permute(0,2,1), batch_y.to(device))\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    total_loss += loss.item()\n",
    "  loss_per_epoch.append(total_loss)\n",
    "  print(f'Epoch: {epoch}/Loss: {total_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(X_test_tensor, tinput)\n",
    "test_loader =  DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "all_labels, predicted = evaluate_model(encoder, decoder, test_loader, device, target_length=horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ =evaluate_baseline(test_loader, target_length=horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_per_epoch)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "predicted = predicted.reshape(-1, 2)\n",
    "all_labels = X_test_tensor\n",
    "num_sequences = batch_y.shape[0]\n",
    "time_points = batch_y.shape[1]\n",
    "predicted_timepoints = predicted.shape[1]\n",
    "\n",
    "fig, axes = plt.subplots(num_sequences, 1, figsize=(10, num_sequences * 3))  # Adjust figsize as needed\n",
    "\n",
    "for i in range(num_sequences):\n",
    "    ax = axes[i]\n",
    "    # Plot actual batch_y values for the entire sequence\n",
    "    ax.scatter(range(time_points), all_labels[i], label='Actual (batch_y)')\n",
    "\n",
    "    # Plot predicted values for the last 2 timepoints\n",
    "    start_predicted_index = time_points - predicted_timepoints\n",
    "    ax.scatter(range(start_predicted_index, time_points), predicted[i], color='red', marker='o', label='Predicted')\n",
    "\n",
    "    ax.set_title(f'Sequence {i+1}')\n",
    "    ax.set_xlabel('Time Point')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(actual_labels, predicted_labels, alpha=0.5)\n",
    "plt.xlabel(\"Actual Y Values\")\n",
    "plt.ylabel(\"Predicted Y Values\")\n",
    "plt.title(\"Scatter Plot of Actual vs. Predicted Y Values\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
